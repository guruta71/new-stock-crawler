"""
Stock Crawler CLI - ë‹¨ì¼ ì§„ì…ì 
"""
import typer
from datetime import date, datetime
from typing import Optional
import pandas as pd
import os
from config import config

app = typer.Typer(help="IPO ë°ì´í„° í¬ë¡¤ëŸ¬ CLI")


def _build_dependencies(headless: bool = config.HEADLESS):
    """
    ì˜ì¡´ì„± ì¡°ë¦½ (DI Container ì—­í• )
    
    Args:
        headless: Playwright í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        
    Returns:
        dict: ì¡°ë¦½ëœ ì˜ì¡´ì„± ê°ì²´ë“¤
    """
    from infra.adapters.utils.console_logger import ConsoleLogger
    from infra.adapters.utils.date_calculator import DateCalculator
    from infra.adapters.web.playwright_page_provider import PlaywrightPageProvider
    from infra.adapters.web.calendar_scraper_adapter import CalendarScraperAdapter
    from infra.adapters.web.detail_scraper_adapter import DetailScraperAdapter
    from infra.adapters.data.dataframe_mapper import DataFrameMapper
    from infra.adapters.data.excel_exporter import ExcelExporter
    from infra.adapters.data.fdr_adapter import FDRAdapter
    from core.services.crawler_service import CrawlerService
    
    # 1. ìœ í‹¸ë¦¬í‹°
    logger = ConsoleLogger()
    date_calculator = DateCalculator()
    
    # 2. Data
    fdr_adapter = FDRAdapter()
    data_mapper = DataFrameMapper()
    data_exporter = ExcelExporter()  # config ì‚¬ìš©
    
    # 3. Storage
    from infra.adapters.storage.google_drive_adapter import GoogleDriveAdapter
    storage_adapter = GoogleDriveAdapter()
    
    # 4. Web Scraping
    page_provider = PlaywrightPageProvider(headless=headless)
    calendar_scraper = CalendarScraperAdapter()
    detail_scraper = DetailScraperAdapter(
        logger=logger,
        ticker_mapper=fdr_adapter,
        market_data_provider=fdr_adapter
    )
    
    # 5. Service
    crawler_service = CrawlerService(
        page_provider=page_provider,
        calendar_scraper=calendar_scraper,
        detail_scraper=detail_scraper,
        data_mapper=data_mapper,
        data_exporter=data_exporter,
        date_calculator=date_calculator,
        logger=logger
    )
    
    return {
        'crawler': crawler_service,
        'page_provider': page_provider,
        'logger': logger,
        'fdr': fdr_adapter,
        'exporter': data_exporter,
        'storage': storage_adapter,
    }


@app.command("full")
def full_crawl(
    start_year: int = typer.Option(2020, "--start-year", "-s", help="í¬ë¡¤ë§ ì‹œì‘ ì—°ë„"),
    headless: bool = typer.Option(config.HEADLESS, "--headless/--no-headless", help="í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ"),
):
    """
    ì „ì²´ ê¸°ê°„ í¬ë¡¤ë§ (ì´ˆê¸° ìˆ˜ì§‘ìš©)
    
    ì§€ì •í•œ ì—°ë„ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  IPO ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ê° ê¸°ì—… ìŠ¤í¬ë˜í•‘ ì§í›„ ì¦‰ì‹œ OHLC ë°ì´í„°ë¥¼ FDRë¡œ ì¡°íšŒí•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    deps = _build_dependencies(headless=headless)
    
    try:
        deps['logger'].info("=" * 60)
        deps['logger'].info("ğŸš€ Stock Crawler - ì „ì²´ í¬ë¡¤ë§")
        deps['logger'].info(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {date.today()}")
        deps['logger'].info(f"ğŸ“† í¬ë¡¤ë§ ì‹œì‘ ì—°ë„: {start_year}ë…„")
        deps['logger'].info("ğŸ” í•„í„°: (ìƒì¥) í¬í•¨, ìŠ¤íŒ© ì œì™¸")
        deps['logger'].info("ğŸ’¹ ì‹œì„¸ ì •ë³´: ìë™ ì¶”ê°€ (FDR)")
        deps['logger'].info("=" * 60)
        
        # Playwright ì´ˆê¸°í™”
        deps['page_provider'].setup()
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        yearly_data = deps['crawler'].run(start_year=start_year)
        
        deps['logger'].info("=" * 60)
        deps['logger'].info("ğŸ ëª¨ë“  í¬ë¡¤ë§ ë° ë³´ê°• ì‘ì—… ì™„ë£Œ")
        
        # Google Drive ì—…ë¡œë“œ
        try:
            output_path = config.get_output_path()
            if output_path.exists():
                deps['logger'].info("â˜ï¸  Google Drive ì—…ë¡œë“œ ì‹œì‘...")
                file_id = deps['storage'].upload_file(output_path)
                deps['logger'].info(f"âœ… ì—…ë¡œë“œ ì„±ê³µ (ID: {file_id})")
        except Exception as e:
            deps['logger'].warning(f"âš ï¸  Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        deps['logger'].info("=" * 60)
        
    except KeyboardInterrupt:
        deps['logger'].warning("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        deps['logger'].error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        deps['page_provider'].cleanup()
        deps['logger'].info("\nâœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


@app.command("enrich")
def enrich_data(
    filepath: str = typer.Option(
        str(config.get_output_path()),
        "--file",
        "-f",
        help="ëŒ€ìƒ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ"
    ),
):
    """
    ê¸°ì¡´ ë°ì´í„°ì— OHLC ë³´ê°•
    
    ì´ë¯¸ ìˆ˜ì§‘ëœ ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ì„œ OHLC ë°ì´í„°ì™€ ìˆ˜ìµë¥ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    run_enrichment.pyë¥¼ ëŒ€ì²´í•˜ëŠ” ì»¤ë§¨ë“œì…ë‹ˆë‹¤.
    """
    from core.services.enrichment_service import EnrichmentService
    from infra.adapters.data.fdr_adapter import FDRAdapter
    from infra.adapters.data.excel_exporter import ExcelExporter
    from infra.adapters.utils.console_logger import ConsoleLogger
    from infra.adapters.storage.google_drive_adapter import GoogleDriveAdapter
    
    logger = ConsoleLogger()
    
    logger.info("=" * 60)
    logger.info("ğŸ“ˆ ì‹œì„¸ ë³´ê°• ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ëŒ€ìƒ íŒŒì¼: {filepath}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(filepath):
        logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        logger.info("ğŸ’¡ íŒ: ë¨¼ì € í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš” (uv run crawler full)")
        raise typer.Exit(code=1)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    logger.info(f"[ì •ë³´] ê¸°ì¡´ ë°ì´í„° ë¡œë”© ì¤‘: {filepath}")
    excel_file = pd.ExcelFile(filepath)
    yearly_data = {}
    
    for sheet_name in excel_file.sheet_names:
        try:
            year = int(sheet_name)
            df = pd.read_excel(filepath, sheet_name=sheet_name)
            yearly_data[year] = df
            logger.info(f"    - [{year}ë…„] {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ")
        except ValueError:
            logger.info(f"    - [ê²½ê³ ] ì‹œíŠ¸ ì´ë¦„ '{sheet_name}'ì€(ëŠ”) ì—°ë„ê°€ ì•„ë‹ˆë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
    
    if not yearly_data:
        logger.warning("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise typer.Exit(code=1)
    
    # Enrichment ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    fdr_adapter = FDRAdapter()
    data_exporter = ExcelExporter()  # config ì‚¬ìš©
    storage_adapter = GoogleDriveAdapter()
    
    enrichment_service = EnrichmentService(
        ticker_mapper=fdr_adapter,
        market_data_provider=fdr_adapter,
        data_exporter=data_exporter,
        logger=logger
    )
    
    # ë³´ê°• ì‘ì—… ì‹¤í–‰
    enrichment_service.enrich_data(yearly_data)
    
    logger.info("=" * 60)
    logger.info("ğŸ ë³´ê°• ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ")
    
    # Google Drive ì—…ë¡œë“œ
    try:
        output_path = Path(filepath)
        logger.info("â˜ï¸  Google Drive ì—…ë¡œë“œ ì‹œì‘...")
        file_id = storage_adapter.upload_file(output_path)
        logger.info(f"âœ… ì—…ë¡œë“œ ì„±ê³µ (ID: {file_id})")
    except Exception as e:
        logger.warning(f"âš ï¸  Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        
    logger.info("=" * 60)


@app.command("daily")
def daily_update(
    target_date: Optional[str] = typer.Option(
        None,
        "--date",
        "-d",
        help="ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹), ê¸°ë³¸ê°’: ì˜¤ëŠ˜"
    ),
    headless: bool = typer.Option(config.HEADLESS, "--headless/--no-headless", help="í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ"),
):
    """
    ì¼ì¼ ì—…ë°ì´íŠ¸ (GitHub Actionsìš©)
    
    íŠ¹ì • ë‚ ì§œì˜ IPO ë°ì´í„°ë§Œ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì¡´ ì—‘ì…€ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    ë‚ ì§œë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    # ë‚ ì§œ íŒŒì‹±
    if target_date:
        try:
            parsed_date = datetime.strptime(target_date, "%Y-%m-%d").date()
        except ValueError:
            typer.echo("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            raise typer.Exit(code=1)
    else:
        parsed_date = date.today()
    
    deps = _build_dependencies(headless=headless)
    
    try:
        deps['logger'].info("=" * 60)
        deps['logger'].info("ğŸ“… Stock Crawler - ì¼ì¼ ì—…ë°ì´íŠ¸")
        deps['logger'].info(f"ëŒ€ìƒ ë‚ ì§œ: {parsed_date}")
        deps['logger'].info("=" * 60)
        
        # Playwright ì´ˆê¸°í™”
        deps['page_provider'].setup()
        
        # ì¼ì¼ í¬ë¡¤ë§ ì‹¤í–‰
        new_data = deps['crawler'].run_daily(target_date=parsed_date)
        
        if new_data:
            total_count = sum(len(df) for df in new_data.values())
            deps['logger'].info(f"âœ… {total_count}ê±´ ì¶”ê°€ë¨")
        else:
            deps['logger'].info("â„¹ï¸  ì˜¤ëŠ˜ì€ ìƒì¥ ì˜ˆì • ì—†ìŒ")
        
        deps['logger'].info("=" * 60)
        deps['logger'].info("ğŸ ì¼ì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # Google Drive ì—…ë¡œë“œ (ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œë§Œ)
        if new_data:
            try:
                output_path = config.get_output_path()
                if output_path.exists():
                    deps['logger'].info("â˜ï¸  Google Drive ì—…ë¡œë“œ ì‹œì‘...")
                    file_id = deps['storage'].upload_file(output_path)
                    deps['logger'].info(f"âœ… ì—…ë¡œë“œ ì„±ê³µ (ID: {file_id})")
            except Exception as e:
                deps['logger'].warning(f"âš ï¸  Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        deps['logger'].info("=" * 60)
        
    except KeyboardInterrupt:
        deps['logger'].warning("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        deps['logger'].error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        deps['page_provider'].cleanup()
        deps['logger'].info("\nâœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    app()
