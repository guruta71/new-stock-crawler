# src/main.py
"""
Stock Crawler Main Application
ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ê¸°ë°˜ (Clean Architecture + Hexagonal)
"""
from datetime import date

# Core
from core.services.crawler_service import CrawlerService
from core.services.enrichment_service import EnrichmentService

# Adapters - Web Scraping
from infra.adapters.web.playwright_page_provider import PlaywrightPageProvider
from infra.adapters.web.calendar_scraper_adapter import CalendarScraperAdapter
from infra.adapters.web.detail_scraper_adapter import DetailScraperAdapter

# Adapters - Data
from infra.adapters.data.dataframe_mapper import DataFrameMapper
from infra.adapters.data.fdr_adapter import FDRAdapter
from infra.adapters.excel_persistence_adapter import LocalExcelPersistenceAdapter

# Adapters - Utilities
from infra.adapters.utils.console_logger import ConsoleLogger
from infra.adapters.utils.date_calculator import DateCalculator


def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì """
    
    # ì„¤ì •
    START_YEAR = 2020
    HEADLESS = True
    
    # ========================================
    # ì˜ì¡´ì„± ìƒì„± (Dependency Injection)
    # ========================================
    
    # 1. ìœ í‹¸ë¦¬í‹°
    logger = ConsoleLogger()
    date_calculator = DateCalculator()
    
    # 2. Web Scraping
    page_provider = PlaywrightPageProvider(headless=HEADLESS)
    calendar_scraper = CalendarScraperAdapter()
    detail_scraper = DetailScraperAdapter(logger=logger)
    
    # 3. Data
    data_mapper = DataFrameMapper()
    data_exporter = LocalExcelPersistenceAdapter()
    
    # 4. Service (ëª¨ë“  ì˜ì¡´ì„± ì£¼ì…)
    crawler_service = CrawlerService(
        page_provider=page_provider,
        calendar_scraper=calendar_scraper,
        detail_scraper=detail_scraper,
        data_mapper=data_mapper,
        data_exporter=data_exporter,
        date_calculator=date_calculator,
        logger=logger
    )
    
    # 5. Enrichment Service
    fdr_adapter = FDRAdapter()
    enrichment_service = EnrichmentService(
        ticker_mapper=fdr_adapter,
        market_data_provider=fdr_adapter,
        data_exporter=data_exporter,
        logger=logger
    )
    
    # ========================================
    # í¬ë¡¤ë§ ì‹¤í–‰
    # ========================================
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸš€ Stock Crawler ì‹œì‘")
        logger.info(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {date.today()}")
        logger.info(f"ğŸ“† í¬ë¡¤ë§ ì‹œì‘ ì—°ë„: {START_YEAR}ë…„")
        logger.info("ğŸ” í•„í„°: (ìƒì¥) í¬í•¨, ìŠ¤íŒ© ì œì™¸")
        logger.info("=" * 60)
        
        # Playwright ì´ˆê¸°í™”
        page_provider.setup()
        
        # 1. í¬ë¡¤ë§ ì‹¤í–‰
        yearly_data = crawler_service.run(start_year=START_YEAR)
        
        # 2. ë°ì´í„° ë³´ê°• (ì‹œì„¸ ë° ì„±ì¥ë¥ )
        if yearly_data:
            enrichment_service.enrich_data(yearly_data)
        
        logger.info("=" * 60)
        logger.info("ğŸ ëª¨ë“  í¬ë¡¤ë§ ë° ë³´ê°• ì‘ì—… ì™„ë£Œ")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        page_provider.cleanup()
        logger.info("\nâœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()