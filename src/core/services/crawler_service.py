"""
크롤링 비즈니스 로직 서비스
"""
from datetime import date
from typing import Dict, List
import pandas as pd

from core.ports.web_scraping_ports import PageProvider, CalendarScraperPort, DetailScraperPort
from core.ports.data_ports import DataMapperPort, DataExporterPort
from core.ports.utility_ports import DateRangeCalculatorPort, LoggerPort
from core.domain.models import StockInfo


class CrawlerService:
    """
    크롤링 워크플로우 오케스트레이션
    
    원칙 준수:
    - 포트만 의존 (어댑터 직접 참조 X)
    - 비즈니스 로직만 포함
    - 모든 의존성을 명시적으로 주입받음
    """
    
    def __init__(
        self,
        page_provider: PageProvider,
        calendar_scraper: CalendarScraperPort,
        detail_scraper: DetailScraperPort,
        data_mapper: DataMapperPort,
        data_exporter: DataExporterPort,
        date_calculator: DateRangeCalculatorPort,
        logger: LoggerPort
    ):
        # 모든 의존성을 생성자에서 받음 (명시적)
        self.page_provider = page_provider
        self.calendar_scraper = calendar_scraper
        self.detail_scraper = detail_scraper
        self.data_mapper = data_mapper
        self.data_exporter = data_exporter
        self.date_calculator = date_calculator
        self.logger = logger
    
    def run(self, start_year: int) -> None:
        """
        크롤링 실행
        
        흐름:
        1. 날짜 범위 계산
        2. 연도별 크롤링
        3. 데이터 저장
        """
        self.logger.info("크롤링 시작")
        
        # 1. 날짜 범위 계산 (비즈니스 로직)
        date_ranges = self.date_calculator.calculate(start_year, date.today())
        
        # 2. Page 객체 준비
        page = self.page_provider.get_page()
        
        # 3. 연도별 크롤링
        yearly_data: Dict[int, pd.DataFrame] = {}
        
        for year, date_range in date_ranges.items():
            self.logger.info(f"[{year}년] 크롤링 시작")
            
            # 3-1. 캘린더에서 IPO 목록 수집
            report = self.calendar_scraper.scrape_calendar(
                page=page,
                year=year,
                start_month=date_range.start_month,
                end_month=date_range.end_month,
                today_day=date_range.day_limit
            )
            
            self.logger.info(
                f"[{year}년] {report.final_stock_count}개 종목 발견 "
                f"(스팩 {report.spack_filtered_count}개 제외)"
            )
            
            if not report.results:
                continue
            
            # 3-2. 상세 정보 수집
            stock_details = self.detail_scraper.scrape_details(
                page=page,
                stocks=report.results
            )
            
            # 3-3. DataFrame 변환
            df = self.data_mapper.to_dataframe(stock_details)
            
            if not df.empty:
                yearly_data[year] = df
                self.logger.info(f"[{year}년] {len(df)}건 수집 완료")
        
        # 4. 데이터 저장
        if yearly_data:
            self.data_exporter.export(yearly_data)
            self.logger.info("저장 완료")
        else:
            self.logger.warning("저장할 데이터 없음")
